{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.utils.data as data\n",
    "\n",
    "train = pd.read_csv(\"data.csv\",parse_dates=[\"Date\"])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "wind1 = pd.read_csv(\"./dataset/wind1.csv\")\n",
    "wind2 = pd.read_csv(\"./dataset/wind2.csv\")\n",
    "wind3 = pd.read_csv(\"./dataset/wind3.csv\")\n",
    "wind4 = pd.read_csv(\"./dataset/wind4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "sequence_length = 20\n",
    "input_size = 2\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 4\n",
    "batch_size = 10\n",
    "num_epochs = 50\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydata(data.Dataset):\n",
    "    def __init__(self, input_dat, y,seq_length):\n",
    "        self.input_dat = torch.tensor(input_dat).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.seq_length - 1:\n",
    "            i_start = idx - self.seq_length + 1\n",
    "            x = self.input_dat[i_start:(idx-3),:]\n",
    "            y = self.y[(idx-3):(idx+1)]\n",
    "        elif idx>=3:\n",
    "            padding = self.input_dat[0].repeat(self.seq_length-idx-1,1)\n",
    "            x = self.input_dat[0:(idx-3),:]\n",
    "            x = torch.cat((padding,x),0)\n",
    "            y = self.y[(idx-3):(idx+1)]\n",
    "        else:\n",
    "            x = self.input_dat[0].repeat(self.seq_length-4,1)\n",
    "            padding_y = self.y[0].repeat(3-idx)\n",
    "            y = self.y[0:(idx+1)]\n",
    "            y = torch.cat((padding_y,y),0)\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.input_dat.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using two strategies,\n",
    "\n",
    "1. Test using MA data and Test relative error using MA data\n",
    "2. Test using original data and test re using original data\n",
    "\n",
    "Later, I'll figure out how to test using original data\n",
    "\n",
    "Split the 4th dataset to test set and the first 3 dataset as training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Sets\n",
    "train1 = np.array(wind1[[\"GridPowerMA\",\"WindSpdMA\"]][19:])\n",
    "train_data1 = mydata(train1, train1[:,1], sequence_length)\n",
    "train2 = np.array(wind2[[\"GridPowerMA\",\"WindSpdMA\"]][19:])\n",
    "train_data2 = mydata(train2, train2[:,1], sequence_length)\n",
    "train3 = np.array(wind3[[\"GridPowerMA\",\"WindSpdMA\"]][19:])\n",
    "train_data3 = mydata(train3, train3[:,1], sequence_length)\n",
    "\n",
    "# Test Sets\n",
    "test = np.array(wind4[[\"GridPowerMA\",\"WindSpdMA\"]][19:])\n",
    "test_target = np.array(wind4[\"GridPower\"][19:])\n",
    "test_data = mydata(test, test_target, sequence_length)\n",
    "test_data1 = mydata(test,test[:,1],sequence_length)\n",
    "\n",
    "# Data Loader\n",
    "train_dat1 = data.DataLoader(dataset=train_data1, batch_size=batch_size, shuffle=True)\n",
    "train_dat2 = data.DataLoader(dataset=train_data2, batch_size=batch_size, shuffle=True)\n",
    "train_dat3 = data.DataLoader(dataset=train_data3, batch_size=batch_size, shuffle=True)\n",
    "test_dat = data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "test_dat1 = data.DataLoader(dataset=test_data1, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Build RNN(LSTM)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "lstm_ma = RNN(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "lstm_o = RNN(input_size,hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "op_ma = torch.optim.Adam(lstm_ma.parameters(), lr=learning_rate)\n",
    "op_o = torch.optim.Adam(lstm_ma.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14612074196338654\n",
      "0.17041724920272827\n",
      "0.03863799571990967\n",
      "0.0016904885414987803\n",
      "0.01225659716874361\n",
      "0.02867770567536354\n",
      "0.0033752568997442722\n",
      "0.002689049346372485\n",
      "0.0019973893649876118\n",
      "0.0041188341565430164\n",
      "0.0019293560180813074\n",
      "0.0006379238329827785\n",
      "0.006164403632283211\n",
      "0.0014794185990467668\n",
      "0.001406988361850381\n",
      "0.0005143170128576458\n",
      "0.0034693307243287563\n",
      "0.0027775708585977554\n",
      "0.0017191050574183464\n",
      "0.0005371536826714873\n",
      "0.005472785327583551\n",
      "0.004536864347755909\n",
      "0.001497647725045681\n",
      "0.0005971401114948094\n",
      "0.0016517939511686563\n",
      "0.001468162750825286\n",
      "0.0015611464623361826\n",
      "0.0008920500986278057\n",
      "0.002659273101016879\n",
      "0.0032231914810836315\n",
      "0.00044612440979108214\n",
      "0.0004657024110201746\n",
      "0.0028955754823982716\n",
      "0.0007331904489547014\n",
      "0.0010867404052987695\n",
      "0.0005081298877485096\n",
      "0.0022646277211606503\n",
      "0.0007999836234375834\n",
      "0.0007833450799807906\n",
      "0.0004945441032759845\n",
      "0.0028037051670253277\n",
      "0.003067599842324853\n",
      "0.002731767948716879\n",
      "0.0004918597987852991\n",
      "0.001753497519530356\n",
      "0.0018597164889797568\n",
      "0.001466825371608138\n",
      "0.0007672790670767426\n",
      "0.0021991704124957323\n",
      "0.0022033629938960075\n",
      "0.001392559614032507\n",
      "0.00040364888263866305\n",
      "0.0028969645500183105\n",
      "0.0017905697459354997\n",
      "0.002079674508422613\n",
      "0.00036537781124934554\n",
      "0.002764584496617317\n",
      "0.000959375873208046\n",
      "0.0012528871884569526\n",
      "0.0005373209714889526\n",
      "0.00230184243991971\n",
      "0.0017131746280938387\n",
      "0.001054756692610681\n",
      "0.0004117240314371884\n",
      "0.0009098504669964314\n",
      "0.001634037122130394\n",
      "0.0027044471353292465\n",
      "0.000326737790601328\n",
      "0.0015092166140675545\n",
      "0.0032581095583736897\n",
      "0.0010103617096319795\n",
      "0.0005200068699195981\n",
      "0.0008881537360139191\n",
      "0.0014434844488278031\n",
      "0.0010708326008170843\n",
      "0.0005681858165189624\n",
      "0.0014925216091796756\n",
      "0.0015310881426557899\n",
      "0.0007072299486026168\n",
      "0.0003520293685141951\n",
      "0.0019967672415077686\n",
      "0.0009852536022663116\n",
      "0.0009602580103091896\n",
      "0.0006410875357687473\n",
      "0.0007744968170300126\n",
      "0.0013548547867685556\n",
      "0.0008852470782585442\n",
      "0.0004226640739943832\n",
      "0.0021612371783703566\n",
      "0.0018623073119670153\n",
      "0.00047890114365145564\n",
      "0.00041688900091685355\n",
      "0.00045024807332083583\n",
      "0.0006080016610212624\n",
      "0.0007685370510444045\n",
      "0.00031672752811573446\n",
      "0.0014048865996301174\n",
      "0.0005517879617400467\n",
      "0.0009292374597862363\n",
      "0.00027583097107708454\n",
      "0.0017326448578387499\n",
      "0.0005996894324198365\n",
      "0.00045895003131590784\n",
      "0.00017359682533424348\n",
      "0.0010787860956043005\n",
      "0.0002722625504247844\n",
      "0.00036558922147378325\n",
      "0.00026229250943288207\n",
      "0.0005892044282518327\n",
      "0.0006025740876793861\n",
      "0.0002227310324087739\n",
      "0.00023961257829796523\n",
      "0.0008860562229529023\n",
      "0.0007117096101865172\n",
      "0.0006704784464091063\n",
      "0.00018985422502737492\n",
      "0.0007310780929401517\n",
      "0.0009745461866259575\n",
      "0.00032331355032511055\n",
      "0.00015793346392456442\n",
      "0.0005171982338652015\n",
      "0.0007765752379782498\n",
      "0.0006043417961336672\n",
      "0.00022976491891313344\n",
      "0.0009575754520483315\n",
      "0.0007985756965354085\n",
      "0.0003047176287509501\n",
      "0.0003236804623156786\n",
      "0.00060232263058424\n",
      "0.00029693893156945705\n",
      "0.00028369363280944526\n",
      "0.0003059760492760688\n",
      "0.0005436526844277978\n",
      "0.00067659851629287\n",
      "0.0005380231887102127\n",
      "0.0001650696649448946\n",
      "0.001681756810285151\n",
      "0.0007133505423553288\n",
      "0.00017226129421032965\n",
      "0.00018567321239970624\n",
      "0.00030600588070228696\n",
      "0.0015815891092643142\n",
      "0.0003137177845928818\n",
      "0.00021760418894700706\n",
      "0.00029859860660508275\n",
      "0.0002157263515982777\n",
      "0.00022574370086658746\n",
      "0.0001911288854898885\n",
      "0.0011824092362076044\n",
      "0.00017035620112437755\n",
      "0.00021682404621969908\n",
      "0.0002276286977576092\n",
      "0.0006176197784952819\n",
      "0.0002890206524170935\n",
      "0.00012441049329936504\n",
      "0.00031044980278238654\n",
      "0.0010859095491468906\n",
      "0.0003356238594278693\n",
      "0.00030097487615421414\n",
      "4.436564995558001e-05\n",
      "0.0011409725993871689\n",
      "0.0007892184075899422\n",
      "0.00032648263731971383\n",
      "0.00010999736696248874\n",
      "0.00035656633554026484\n",
      "0.0005139852873980999\n",
      "8.996319229481742e-05\n",
      "0.0002292779681738466\n",
      "0.0003743753186427057\n",
      "0.0004288415366318077\n",
      "0.00020733955898322165\n",
      "7.878239557612687e-05\n",
      "0.00038799349567852914\n",
      "0.0011280433973297477\n",
      "0.00045724055962637067\n",
      "8.673285628901795e-05\n",
      "0.0014853253960609436\n",
      "0.00029741122853010893\n",
      "0.00022109600831754506\n",
      "9.290855086874217e-05\n",
      "0.00020379755005706102\n",
      "0.000404429534683004\n",
      "0.00034555321326479316\n",
      "7.683435978833586e-05\n",
      "0.00047135126078501344\n",
      "0.0005688412929885089\n",
      "0.0003448162751737982\n",
      "0.00017492810729891062\n",
      "0.00038081195089034736\n",
      "9.459746070206165e-05\n",
      "0.0002521692658774555\n",
      "0.00010835466673597693\n",
      "0.0006634477758780122\n",
      "0.0004564998671412468\n",
      "0.0002673353592399508\n",
      "0.0001073295934475027\n",
      "0.0004518796631600708\n",
      "0.0004288962227292359\n",
      "5.833940304000862e-05\n",
      "0.00022598161012865603\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (value,labels) in enumerate(train_dat1):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm_ma(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op_ma.zero_grad()\n",
    "        loss.backward()\n",
    "        op_ma.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())\n",
    "    \n",
    "    for i, (value,labels) in enumerate(train_dat2):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm_ma(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op_ma.zero_grad()\n",
    "        loss.backward()\n",
    "        op_ma.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())\n",
    "    \n",
    "    for i, (value,labels) in enumerate(train_dat3):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm_ma(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op_ma.zero_grad()\n",
    "        loss.backward()\n",
    "        op_ma.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0419)\n",
      "0.1605526801002653\n",
      "0.7101374864578247\n",
      "0.15656882524490356\n",
      "0.4780701754385965\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "lstm_ma.eval()\n",
    "mse = 0\n",
    "i = 0\n",
    "re = np.array([])\n",
    "with torch.no_grad():\n",
    "    for values, labels in test_dat:\n",
    "        values = values.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = lstm_ma(values)\n",
    "        mse += torch.sum((outputs-labels)**2)\n",
    "        i += len(labels.flatten())\n",
    "        re_tmp = np.array(np.abs(labels.flatten()-outputs.flatten()))\n",
    "        re = np.concatenate((re,re_tmp),axis=0)\n",
    "print(mse/i)\n",
    "mean_rerror = np.mean(re)\n",
    "max_rerror = np.max(re)\n",
    "median_rerror = np.median(re)\n",
    "print(mean_rerror)\n",
    "print(max_rerror)\n",
    "print(median_rerror)\n",
    "idx = re <= 0.15\n",
    "idx = idx + 0\n",
    "print(sum(idx)/len(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0004)\n",
      "0.013507568038869322\n",
      "0.09291854500770569\n",
      "0.0100894495844841\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluation using ma data\n",
    "lstm_ma.eval()\n",
    "mse = 0\n",
    "i = 0\n",
    "re = np.array([])\n",
    "with torch.no_grad():\n",
    "    for values, labels in test_dat1:\n",
    "        values = values.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = lstm_ma(values)\n",
    "        mse += torch.sum((outputs-labels)**2)\n",
    "        i += len(labels.flatten())\n",
    "        re_tmp = np.array(np.abs(labels.flatten()-outputs.flatten()))\n",
    "        re = np.concatenate((re,re_tmp),axis=0)\n",
    "print(mse/i)\n",
    "mean_rerror = np.mean(re)\n",
    "max_rerror = np.max(re)\n",
    "median_rerror = np.median(re)\n",
    "print(mean_rerror)\n",
    "print(max_rerror)\n",
    "print(median_rerror)\n",
    "idx = re <= 0.15\n",
    "idx = idx + 0\n",
    "print(sum(idx)/len(idx))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If just input the original data, let's see what will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Sets\n",
    "train1 = np.array(wind1[[\"GridPower\",\"WindSpd\"]][19:])\n",
    "train_data1 = mydata(train1, train1[:,1], sequence_length)\n",
    "train2 = np.array(wind2[[\"GridPower\",\"WindSpdMA\"]][19:])\n",
    "train_data2 = mydata(train2, train2[:,1], sequence_length)\n",
    "train3 = np.array(wind3[[\"GridPower\",\"WindSpd\"]][19:])\n",
    "train_data3 = mydata(train3, train3[:,1], sequence_length)\n",
    "\n",
    "# Data Loader\n",
    "train_dat1 = data.DataLoader(dataset=train_data1, batch_size=batch_size, shuffle=True)\n",
    "train_dat2 = data.DataLoader(dataset=train_data2, batch_size=batch_size, shuffle=True)\n",
    "train_dat3 = data.DataLoader(dataset=train_data3, batch_size=batch_size, shuffle=True)\n",
    "test_dat = data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "test_dat1 = data.DataLoader(dataset=test_data1, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1531873345375061\n",
      "0.12544545531272888\n",
      "0.0009025622857734561\n",
      "0.2077295482158661\n",
      "0.07843682914972305\n",
      "0.0010841478360816836\n",
      "0.086118683218956\n",
      "0.12809598445892334\n",
      "0.0031815648544579744\n",
      "0.08365408331155777\n",
      "0.11831847578287125\n",
      "0.000769954058341682\n",
      "0.2051692008972168\n",
      "0.08468858897686005\n",
      "0.0014968625036999583\n",
      "0.1295761913061142\n",
      "0.13531726598739624\n",
      "0.0007499171188101172\n",
      "0.15072233974933624\n",
      "0.1124248281121254\n",
      "0.00595102971419692\n",
      "0.16093870997428894\n",
      "0.1186675876379013\n",
      "0.006633176002651453\n",
      "0.13628099858760834\n",
      "0.09450796991586685\n",
      "0.015297988429665565\n",
      "0.1525070071220398\n",
      "0.08831898868083954\n",
      "0.013849550858139992\n",
      "0.1587451696395874\n",
      "0.09828118234872818\n",
      "0.0028796123806387186\n",
      "0.12943123281002045\n",
      "0.0926542654633522\n",
      "0.0030538467690348625\n",
      "0.1430230438709259\n",
      "0.0819411650300026\n",
      "0.0010952467564493418\n",
      "0.24250474572181702\n",
      "0.08089534193277359\n",
      "0.0019500041380524635\n",
      "0.24121515452861786\n",
      "0.10727778822183609\n",
      "0.0010666244197636843\n",
      "0.12220825999975204\n",
      "0.08426358550786972\n",
      "0.0014305065851658583\n",
      "0.20956949889659882\n",
      "0.102442167699337\n",
      "0.0011334579903632402\n",
      "0.21401607990264893\n",
      "0.08752311766147614\n",
      "0.0018884253222495317\n",
      "0.14680470526218414\n",
      "0.09655367583036423\n",
      "0.004684622399508953\n",
      "0.17163226008415222\n",
      "0.09223904460668564\n",
      "0.006383763160556555\n",
      "0.14094670116901398\n",
      "0.1193002238869667\n",
      "0.0032496261410415173\n",
      "0.1781582087278366\n",
      "0.10271594673395157\n",
      "0.004702751059085131\n",
      "0.14076003432273865\n",
      "0.07834679633378983\n",
      "0.002870944794267416\n",
      "0.09791184961795807\n",
      "0.08957894146442413\n",
      "0.002412188798189163\n",
      "0.1700272411108017\n",
      "0.08928106725215912\n",
      "0.003302164375782013\n",
      "0.11916115134954453\n",
      "0.09666270762681961\n",
      "0.00189276784658432\n",
      "0.11594971269369125\n",
      "0.0833592340350151\n",
      "0.0022317287512123585\n",
      "0.12531794607639313\n",
      "0.11079897731542587\n",
      "0.0011657570721581578\n",
      "0.16109327971935272\n",
      "0.1252366602420807\n",
      "0.0024370583705604076\n",
      "0.18514873087406158\n",
      "0.08833211660385132\n",
      "0.000453971850220114\n",
      "0.1669015735387802\n",
      "0.10416261851787567\n",
      "0.0006922614411450922\n",
      "0.18009448051452637\n",
      "0.11447994410991669\n",
      "0.00102960504591465\n",
      "0.09660296142101288\n",
      "0.09306664764881134\n",
      "0.002239647787064314\n",
      "0.1285443753004074\n",
      "0.0873933732509613\n",
      "0.001850192085839808\n",
      "0.12542566657066345\n",
      "0.10813460499048233\n",
      "0.0027317749336361885\n",
      "0.1751789003610611\n",
      "0.09692458808422089\n",
      "0.005348438862711191\n",
      "0.17215701937675476\n",
      "0.1066652163863182\n",
      "0.002957900520414114\n",
      "0.11947337538003922\n",
      "0.07967443764209747\n",
      "0.0017116771778091788\n",
      "0.1882651299238205\n",
      "0.0784744992852211\n",
      "0.0028897218871861696\n",
      "0.11896934360265732\n",
      "0.12967434525489807\n",
      "0.002056162804365158\n",
      "0.1190938726067543\n",
      "0.11743178218603134\n",
      "0.0016921423375606537\n",
      "0.0882204994559288\n",
      "0.10421391576528549\n",
      "0.0012552604312077165\n",
      "0.12388636916875839\n",
      "0.09793852269649506\n",
      "0.0018227057298645377\n",
      "0.22079066932201385\n",
      "0.08562903106212616\n",
      "0.0016346757765859365\n",
      "0.1882796585559845\n",
      "0.09363500773906708\n",
      "0.0028056777082383633\n",
      "0.14204619824886322\n",
      "0.08925428986549377\n",
      "0.001468768692575395\n",
      "0.18664126098155975\n",
      "0.1030011922121048\n",
      "0.002617416437715292\n",
      "0.11598779261112213\n",
      "0.08585212379693985\n",
      "0.0011321225902065635\n",
      "0.15419378876686096\n",
      "0.08347578346729279\n",
      "0.0018658122280612588\n",
      "0.18332841992378235\n",
      "0.09236332774162292\n",
      "0.0027549355290830135\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (value,labels) in enumerate(train_dat1):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm_o(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op_o.zero_grad()\n",
    "        loss.backward()\n",
    "        op_o.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())\n",
    "    \n",
    "    for i, (value,labels) in enumerate(train_dat2):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm_o(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op_o.zero_grad()\n",
    "        loss.backward()\n",
    "        op_o.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())\n",
    "    \n",
    "    for i, (value,labels) in enumerate(train_dat3):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm_ma(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op_o.zero_grad()\n",
    "        loss.backward()\n",
    "        op_o.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0527)\n",
      "0.17110022030640068\n",
      "0.7066285610198975\n",
      "0.14498671144247055\n",
      "0.5184210526315789\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "lstm_ma.eval()\n",
    "mse = 0\n",
    "i = 0\n",
    "re = np.array([])\n",
    "with torch.no_grad():\n",
    "    for values, labels in test_dat:\n",
    "        values = values.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = lstm_ma(values)\n",
    "        mse += torch.sum((outputs-labels)**2)\n",
    "        i += len(labels.flatten())\n",
    "        re_tmp = np.array(np.abs(labels.flatten()-outputs.flatten()))\n",
    "        re = np.concatenate((re,re_tmp),axis=0)\n",
    "print(mse/i)\n",
    "mean_rerror = np.mean(re)\n",
    "max_rerror = np.max(re)\n",
    "median_rerror = np.median(re)\n",
    "print(mean_rerror)\n",
    "print(max_rerror)\n",
    "print(median_rerror)\n",
    "idx = re <= 0.15\n",
    "idx = idx + 0\n",
    "print(sum(idx)/len(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "sequence_length = 20\n",
    "input_size = 2\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 4\n",
    "batch_size = 10\n",
    "num_epochs = 500\n",
    "learning_rate = 5e-5\n",
    "\n",
    "lstm = RNN(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "op = torch.optim.Adam(lstm_ma.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Sets\n",
    "train1 = np.array(wind1[[\"GridPowerMA\",\"WindSpdMA\"]][19:])\n",
    "train_data1 = mydata(train1, train1[:,1], sequence_length)\n",
    "train2 = np.array(wind2[[\"GridPowerMA\",\"WindSpdMA\"]][19:])\n",
    "train_data2 = mydata(train2, train2[:,1], sequence_length)\n",
    "train3 = np.array(wind3[[\"GridPowerMA\",\"WindSpdMA\"]][19:])\n",
    "train_data3 = mydata(train3, train3[:,1], sequence_length)\n",
    "\n",
    "# Test Sets\n",
    "test = np.array(wind4[[\"GridPowerMA\",\"WindSpdMA\"]][19:])\n",
    "test_target = np.array(wind4[\"GridPower\"][19:])\n",
    "test_data = mydata(test, test_target, sequence_length)\n",
    "test_data1 = mydata(test,test[:,1],sequence_length)\n",
    "\n",
    "# Data Loader\n",
    "train_dat1 = data.DataLoader(dataset=train_data1, batch_size=batch_size, shuffle=True)\n",
    "train_dat2 = data.DataLoader(dataset=train_data2, batch_size=batch_size, shuffle=True)\n",
    "train_dat3 = data.DataLoader(dataset=train_data3, batch_size=batch_size, shuffle=True)\n",
    "test_dat = data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "test_dat1 = data.DataLoader(dataset=test_data1, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07628988474607468\n",
      "0.1154961809515953\n",
      "0.06399889290332794\n",
      "0.15725940465927124\n",
      "0.11637923866510391\n",
      "0.07082464545965195\n",
      "0.15220190584659576\n",
      "0.11052770912647247\n",
      "0.0721173882484436\n",
      "0.1282324492931366\n",
      "0.10989866405725479\n",
      "0.07086174190044403\n",
      "0.18796448409557343\n",
      "0.07573775947093964\n",
      "0.0737876296043396\n",
      "0.2390126883983612\n",
      "0.09077264368534088\n",
      "0.07214216887950897\n",
      "0.18955691158771515\n",
      "0.09317359328269958\n",
      "0.07101200520992279\n",
      "0.18718266487121582\n",
      "0.11842825263738632\n",
      "0.07725413888692856\n",
      "0.23381051421165466\n",
      "0.08929286897182465\n",
      "0.09051570296287537\n",
      "0.25904297828674316\n",
      "0.10889600217342377\n",
      "0.06825050711631775\n",
      "0.16577009856700897\n",
      "0.12797948718070984\n",
      "0.08025266975164413\n",
      "0.1806907057762146\n",
      "0.1020045131444931\n",
      "0.07194123417139053\n",
      "0.13029657304286957\n",
      "0.12416546046733856\n",
      "0.06657656282186508\n",
      "0.1630934178829193\n",
      "0.10000459104776382\n",
      "0.069209985435009\n",
      "0.19660945236682892\n",
      "0.14300093054771423\n",
      "0.06579163670539856\n",
      "0.13010874390602112\n",
      "0.09650585055351257\n",
      "0.0682736486196518\n",
      "0.19731338322162628\n",
      "0.11823345720767975\n",
      "0.07502418011426926\n",
      "0.10676491260528564\n",
      "0.09641291946172714\n",
      "0.07517272979021072\n",
      "0.2827063202857971\n",
      "0.11767437309026718\n",
      "0.06958877295255661\n",
      "0.2459404468536377\n",
      "0.12443310022354126\n",
      "0.07695550471544266\n",
      "0.1499703973531723\n",
      "0.14855259656906128\n",
      "0.06905069202184677\n",
      "0.3154045343399048\n",
      "0.12308961153030396\n",
      "0.07758454233407974\n",
      "0.12495344877243042\n",
      "0.1163128986954689\n",
      "0.07294110953807831\n",
      "0.18090827763080597\n",
      "0.10594624280929565\n",
      "0.07728569209575653\n",
      "0.10954705625772476\n",
      "0.12790820002555847\n",
      "0.08350803703069687\n",
      "0.2612045407295227\n",
      "0.1132204532623291\n",
      "0.06707622110843658\n",
      "0.17792728543281555\n",
      "0.12092999368906021\n",
      "0.07820947468280792\n",
      "0.14440301060676575\n",
      "0.09137564152479172\n",
      "0.06835043430328369\n",
      "0.13025085628032684\n",
      "0.1346997171640396\n",
      "0.07848256081342697\n",
      "0.1495145708322525\n",
      "0.10133220255374908\n",
      "0.07109709829092026\n",
      "0.15321114659309387\n",
      "0.10769684612751007\n",
      "0.06589465588331223\n",
      "0.21028800308704376\n",
      "0.12602919340133667\n",
      "0.06869591772556305\n",
      "0.19139723479747772\n",
      "0.13709034025669098\n",
      "0.07289931178092957\n",
      "0.16944774985313416\n",
      "0.1153372973203659\n",
      "0.0704614520072937\n",
      "0.17613919079303741\n",
      "0.08785752952098846\n",
      "0.07827254384756088\n",
      "0.1355465203523636\n",
      "0.1187329888343811\n",
      "0.06827132403850555\n",
      "0.10321587324142456\n",
      "0.12327182292938232\n",
      "0.07966761291027069\n",
      "0.2167360484600067\n",
      "0.09048529714345932\n",
      "0.07721574604511261\n",
      "0.21001437306404114\n",
      "0.13232707977294922\n",
      "0.06451568007469177\n",
      "0.12193727493286133\n",
      "0.13474638760089874\n",
      "0.0720290094614029\n",
      "0.18943551182746887\n",
      "0.09243061393499374\n",
      "0.08358137309551239\n",
      "0.16928617656230927\n",
      "0.10196386277675629\n",
      "0.07954371720552444\n",
      "0.14258266985416412\n",
      "0.09602561593055725\n",
      "0.07273542135953903\n",
      "0.24047645926475525\n",
      "0.08773760497570038\n",
      "0.07064389437437057\n",
      "0.1633746325969696\n",
      "0.1208546906709671\n",
      "0.06995799392461777\n",
      "0.13686077296733856\n",
      "0.12419404089450836\n",
      "0.06758107990026474\n",
      "0.2409011870622635\n",
      "0.11346492916345596\n",
      "0.07371283322572708\n",
      "0.24510438740253448\n",
      "0.11646842956542969\n",
      "0.08038772642612457\n",
      "0.14487935602664948\n",
      "0.15946850180625916\n",
      "0.06924503296613693\n",
      "0.1615145206451416\n",
      "0.13138146698474884\n",
      "0.07845361530780792\n",
      "0.1296847015619278\n",
      "0.1205282211303711\n",
      "0.07827000319957733\n",
      "0.12961454689502716\n",
      "0.1340876668691635\n",
      "0.07554261386394501\n",
      "0.1510009467601776\n",
      "0.1185714453458786\n",
      "0.07083284854888916\n",
      "0.12237656116485596\n",
      "0.09336386620998383\n",
      "0.07448174059391022\n",
      "0.22157903015613556\n",
      "0.10648155212402344\n",
      "0.07076159864664078\n",
      "0.15183933079242706\n",
      "0.12580786645412445\n",
      "0.07001150399446487\n",
      "0.17056462168693542\n",
      "0.10950036346912384\n",
      "0.07476866990327835\n",
      "0.10778504610061646\n",
      "0.09153161197900772\n",
      "0.07223457843065262\n",
      "0.11403729021549225\n",
      "0.1376412808895111\n",
      "0.07006590068340302\n",
      "0.1448163539171219\n",
      "0.10878100246191025\n",
      "0.06442417949438095\n",
      "0.15261031687259674\n",
      "0.14516083896160126\n",
      "0.06834913790225983\n",
      "0.12561510503292084\n",
      "0.11309267580509186\n",
      "0.07007656991481781\n",
      "0.12366241216659546\n",
      "0.0887349396944046\n",
      "0.07450439780950546\n",
      "0.11565940082073212\n",
      "0.10534526407718658\n",
      "0.06553445756435394\n",
      "0.19422481954097748\n",
      "0.14301136136054993\n",
      "0.07251153141260147\n",
      "0.25027740001678467\n",
      "0.07785115391016006\n",
      "0.06928350776433945\n",
      "0.13040633499622345\n",
      "0.1264767348766327\n",
      "0.08256656676530838\n",
      "0.1656942069530487\n",
      "0.09638546407222748\n",
      "0.06369112432003021\n",
      "0.15493719279766083\n",
      "0.10823843628168106\n",
      "0.07424326241016388\n",
      "0.16207656264305115\n",
      "0.13019093871116638\n",
      "0.07923787087202072\n",
      "0.1095421090722084\n",
      "0.11631759256124496\n",
      "0.06765754520893097\n",
      "0.1820131540298462\n",
      "0.1372760832309723\n",
      "0.07053731381893158\n",
      "0.13167794048786163\n",
      "0.09138620644807816\n",
      "0.07441182434558868\n",
      "0.21440072357654572\n",
      "0.12608489394187927\n",
      "0.07770150899887085\n",
      "0.23478050529956818\n",
      "0.14792494475841522\n",
      "0.08115826547145844\n",
      "0.124799445271492\n",
      "0.12378233671188354\n",
      "0.07574370503425598\n",
      "0.25714045763015747\n",
      "0.11461658775806427\n",
      "0.06754133105278015\n",
      "0.22373375296592712\n",
      "0.10225822031497955\n",
      "0.07361333072185516\n",
      "0.13638511300086975\n",
      "0.1374306082725525\n",
      "0.06949035823345184\n",
      "0.2577248811721802\n",
      "0.1288445144891739\n",
      "0.07620798051357269\n",
      "0.16100600361824036\n",
      "0.13479653000831604\n",
      "0.07467259466648102\n",
      "0.15880148112773895\n",
      "0.11261340230703354\n",
      "0.07194573432207108\n",
      "0.2835409939289093\n",
      "0.11031264066696167\n",
      "0.07154566794633865\n",
      "0.21923618018627167\n",
      "0.12909691035747528\n",
      "0.06117505580186844\n",
      "0.18517887592315674\n",
      "0.10370151698589325\n",
      "0.07748465240001678\n",
      "0.2097480297088623\n",
      "0.128983736038208\n",
      "0.0747394934296608\n",
      "0.15349800884723663\n",
      "0.1239699125289917\n",
      "0.08056934177875519\n",
      "0.13433203101158142\n",
      "0.13929563760757446\n",
      "0.07281488925218582\n",
      "0.1604166328907013\n",
      "0.12536592781543732\n",
      "0.0745069831609726\n",
      "0.1477353870868683\n",
      "0.15626060962677002\n",
      "0.06577321887016296\n",
      "0.09427149593830109\n",
      "0.11909797042608261\n",
      "0.07249371707439423\n",
      "0.170139878988266\n",
      "0.14747872948646545\n",
      "0.0796256810426712\n",
      "0.11867542564868927\n",
      "0.11785348504781723\n",
      "0.07624593377113342\n",
      "0.09988735616207123\n",
      "0.10716396570205688\n",
      "0.06846211850643158\n",
      "0.21068091690540314\n",
      "0.09153873473405838\n",
      "0.0759921446442604\n",
      "0.19352707266807556\n",
      "0.10722124576568604\n",
      "0.08072421699762344\n",
      "0.1389043629169464\n",
      "0.10799088329076767\n",
      "0.07703698426485062\n",
      "0.15863075852394104\n",
      "0.10257512331008911\n",
      "0.07526326924562454\n",
      "0.17367973923683167\n",
      "0.14220045506954193\n",
      "0.07740141451358795\n",
      "0.13319626450538635\n",
      "0.12122020870447159\n",
      "0.06821034103631973\n",
      "0.16552260518074036\n",
      "0.1038430705666542\n",
      "0.07131262868642807\n",
      "0.14494025707244873\n",
      "0.1576794683933258\n",
      "0.07368214428424835\n",
      "0.16485925018787384\n",
      "0.10297535359859467\n",
      "0.07428069412708282\n",
      "0.3155912458896637\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Programmes\\Python\\BDAI\\Project\\project.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Programmes/Python/BDAI/Project/project.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# optimization step\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Programmes/Python/BDAI/Project/project.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m op\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Programmes/Python/BDAI/Project/project.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Programmes/Python/BDAI/Project/project.ipynb#X33sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m op\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Programmes/Python/BDAI/Project/project.ipynb#X33sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m500\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (value,labels) in enumerate(train_dat1):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op.zero_grad()\n",
    "        loss.backward()\n",
    "        op.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())\n",
    "    \n",
    "    for i, (value,labels) in enumerate(train_dat2):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op.zero_grad()\n",
    "        loss.backward()\n",
    "        op.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())\n",
    "    \n",
    "    for i, (value,labels) in enumerate(train_dat3):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op.zero_grad()\n",
    "        loss.backward()\n",
    "        op.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6673)\n",
      "0.6081427295731262\n",
      "2.0059587955474854\n",
      "0.48045556247234344\n",
      "0.20350877192982456\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "lstm.eval()\n",
    "mse = 0\n",
    "i = 0\n",
    "re = np.array([])\n",
    "with torch.no_grad():\n",
    "    for values, labels in test_dat:\n",
    "        values = values.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = lstm(values)\n",
    "        mse += torch.sum((outputs-labels)**2)\n",
    "        i += len(labels.flatten())\n",
    "        re_tmp = np.array(np.abs(labels.flatten()-outputs.flatten()))\n",
    "        re = np.concatenate((re,re_tmp),axis=0)\n",
    "print(mse/i)\n",
    "mean_rerror = np.mean(re)\n",
    "max_rerror = np.max(re)\n",
    "median_rerror = np.median(re)\n",
    "print(mean_rerror)\n",
    "print(max_rerror)\n",
    "print(median_rerror)\n",
    "idx = re <= 0.15\n",
    "idx = idx + 0\n",
    "print(sum(idx)/len(idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
