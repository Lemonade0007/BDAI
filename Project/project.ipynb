{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.utils.data as data\n",
    "\n",
    "train = pd.read_csv(\"data.csv\",parse_dates=[\"Date\"])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "wind1 = pd.read_csv(\"./dataset/wind1.csv\")\n",
    "wind2 = pd.read_csv(\"./dataset/wind2.csv\")\n",
    "wind3 = pd.read_csv(\"./dataset/wind3.csv\")\n",
    "wind4 = pd.read_csv(\"./dataset/wind4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "sequence_length = 20\n",
    "input_size = 2\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 4\n",
    "batch_size = 10\n",
    "num_epochs = 50\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydata(data.Dataset):\n",
    "    def __init__(self, input_dat, y,seq_length):\n",
    "        self.input_dat = torch.tensor(input_dat).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.seq_length - 1:\n",
    "            i_start = idx - self.seq_length + 1\n",
    "            x = self.input_dat[i_start:(idx-3),:]\n",
    "            y = self.y[(idx-3):(idx+1)]\n",
    "        elif idx>=3:\n",
    "            padding = self.input_dat[0].repeat(self.seq_length-idx-1,1)\n",
    "            x = self.input_dat[0:(idx-3),:]\n",
    "            x = torch.cat((padding,x),0)\n",
    "            y = self.y[(idx-3):(idx+1)]\n",
    "        else:\n",
    "            x = self.input_dat[0].repeat(self.seq_length-4,1)\n",
    "            padding_y = self.y[0].repeat(3-idx)\n",
    "            y = self.y[0:(idx+1)]\n",
    "            y = torch.cat((padding_y,y),0)\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.input_dat.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using two strategies,\n",
    "\n",
    "1. Test using MA data and Test relative error using MA data\n",
    "2. Test using original data and test re using original data\n",
    "\n",
    "Later, I'll figure out how to test using original data\n",
    "\n",
    "Split the 4th dataset to test set and the first 3 dataset as training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Sets\n",
    "train1 = np.array(wind1[[\"GridPowerMA\",\"WindSpdMA\"]][19:])\n",
    "train_data1 = mydata(train1, train1[:,1], sequence_length)\n",
    "train2 = np.array(wind2[[\"GridPowerMA\",\"WindSpdMA\"]][19:])\n",
    "train_data2 = mydata(train2, train2[:,1], sequence_length)\n",
    "train3 = np.array(wind3[[\"GridPowerMA\",\"WindSpdMA\"]][19:])\n",
    "train_data3 = mydata(train3, train3[:,1], sequence_length)\n",
    "\n",
    "# Test Sets\n",
    "test = np.array(wind4[[\"GridPowerMA\",\"WindSpdMA\"]][19:])\n",
    "test_target = np.array(wind4[\"GridPower\"][19:])\n",
    "test_data = mydata(test, test_target, sequence_length)\n",
    "test_data1 = mydata(test,test[:,1],sequence_length)\n",
    "\n",
    "# Data Loader\n",
    "train_dat1 = data.DataLoader(dataset=train_data1, batch_size=batch_size, shuffle=True)\n",
    "train_dat2 = data.DataLoader(dataset=train_data2, batch_size=batch_size, shuffle=True)\n",
    "train_dat3 = data.DataLoader(dataset=train_data3, batch_size=batch_size, shuffle=True)\n",
    "test_dat = data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "test_dat1 = data.DataLoader(dataset=test_data1, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Build RNN(LSTM)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "lstm_ma = RNN(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "lstm_o = RNN(input_size,hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "op_ma = torch.optim.Adam(lstm_ma.parameters(), lr=learning_rate)\n",
    "op_o = torch.optim.Adam(lstm_ma.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14612074196338654\n",
      "0.17041724920272827\n",
      "0.03863799571990967\n",
      "0.0016904885414987803\n",
      "0.01225659716874361\n",
      "0.02867770567536354\n",
      "0.0033752568997442722\n",
      "0.002689049346372485\n",
      "0.0019973893649876118\n",
      "0.0041188341565430164\n",
      "0.0019293560180813074\n",
      "0.0006379238329827785\n",
      "0.006164403632283211\n",
      "0.0014794185990467668\n",
      "0.001406988361850381\n",
      "0.0005143170128576458\n",
      "0.0034693307243287563\n",
      "0.0027775708585977554\n",
      "0.0017191050574183464\n",
      "0.0005371536826714873\n",
      "0.005472785327583551\n",
      "0.004536864347755909\n",
      "0.001497647725045681\n",
      "0.0005971401114948094\n",
      "0.0016517939511686563\n",
      "0.001468162750825286\n",
      "0.0015611464623361826\n",
      "0.0008920500986278057\n",
      "0.002659273101016879\n",
      "0.0032231914810836315\n",
      "0.00044612440979108214\n",
      "0.0004657024110201746\n",
      "0.0028955754823982716\n",
      "0.0007331904489547014\n",
      "0.0010867404052987695\n",
      "0.0005081298877485096\n",
      "0.0022646277211606503\n",
      "0.0007999836234375834\n",
      "0.0007833450799807906\n",
      "0.0004945441032759845\n",
      "0.0028037051670253277\n",
      "0.003067599842324853\n",
      "0.002731767948716879\n",
      "0.0004918597987852991\n",
      "0.001753497519530356\n",
      "0.0018597164889797568\n",
      "0.001466825371608138\n",
      "0.0007672790670767426\n",
      "0.0021991704124957323\n",
      "0.0022033629938960075\n",
      "0.001392559614032507\n",
      "0.00040364888263866305\n",
      "0.0028969645500183105\n",
      "0.0017905697459354997\n",
      "0.002079674508422613\n",
      "0.00036537781124934554\n",
      "0.002764584496617317\n",
      "0.000959375873208046\n",
      "0.0012528871884569526\n",
      "0.0005373209714889526\n",
      "0.00230184243991971\n",
      "0.0017131746280938387\n",
      "0.001054756692610681\n",
      "0.0004117240314371884\n",
      "0.0009098504669964314\n",
      "0.001634037122130394\n",
      "0.0027044471353292465\n",
      "0.000326737790601328\n",
      "0.0015092166140675545\n",
      "0.0032581095583736897\n",
      "0.0010103617096319795\n",
      "0.0005200068699195981\n",
      "0.0008881537360139191\n",
      "0.0014434844488278031\n",
      "0.0010708326008170843\n",
      "0.0005681858165189624\n",
      "0.0014925216091796756\n",
      "0.0015310881426557899\n",
      "0.0007072299486026168\n",
      "0.0003520293685141951\n",
      "0.0019967672415077686\n",
      "0.0009852536022663116\n",
      "0.0009602580103091896\n",
      "0.0006410875357687473\n",
      "0.0007744968170300126\n",
      "0.0013548547867685556\n",
      "0.0008852470782585442\n",
      "0.0004226640739943832\n",
      "0.0021612371783703566\n",
      "0.0018623073119670153\n",
      "0.00047890114365145564\n",
      "0.00041688900091685355\n",
      "0.00045024807332083583\n",
      "0.0006080016610212624\n",
      "0.0007685370510444045\n",
      "0.00031672752811573446\n",
      "0.0014048865996301174\n",
      "0.0005517879617400467\n",
      "0.0009292374597862363\n",
      "0.00027583097107708454\n",
      "0.0017326448578387499\n",
      "0.0005996894324198365\n",
      "0.00045895003131590784\n",
      "0.00017359682533424348\n",
      "0.0010787860956043005\n",
      "0.0002722625504247844\n",
      "0.00036558922147378325\n",
      "0.00026229250943288207\n",
      "0.0005892044282518327\n",
      "0.0006025740876793861\n",
      "0.0002227310324087739\n",
      "0.00023961257829796523\n",
      "0.0008860562229529023\n",
      "0.0007117096101865172\n",
      "0.0006704784464091063\n",
      "0.00018985422502737492\n",
      "0.0007310780929401517\n",
      "0.0009745461866259575\n",
      "0.00032331355032511055\n",
      "0.00015793346392456442\n",
      "0.0005171982338652015\n",
      "0.0007765752379782498\n",
      "0.0006043417961336672\n",
      "0.00022976491891313344\n",
      "0.0009575754520483315\n",
      "0.0007985756965354085\n",
      "0.0003047176287509501\n",
      "0.0003236804623156786\n",
      "0.00060232263058424\n",
      "0.00029693893156945705\n",
      "0.00028369363280944526\n",
      "0.0003059760492760688\n",
      "0.0005436526844277978\n",
      "0.00067659851629287\n",
      "0.0005380231887102127\n",
      "0.0001650696649448946\n",
      "0.001681756810285151\n",
      "0.0007133505423553288\n",
      "0.00017226129421032965\n",
      "0.00018567321239970624\n",
      "0.00030600588070228696\n",
      "0.0015815891092643142\n",
      "0.0003137177845928818\n",
      "0.00021760418894700706\n",
      "0.00029859860660508275\n",
      "0.0002157263515982777\n",
      "0.00022574370086658746\n",
      "0.0001911288854898885\n",
      "0.0011824092362076044\n",
      "0.00017035620112437755\n",
      "0.00021682404621969908\n",
      "0.0002276286977576092\n",
      "0.0006176197784952819\n",
      "0.0002890206524170935\n",
      "0.00012441049329936504\n",
      "0.00031044980278238654\n",
      "0.0010859095491468906\n",
      "0.0003356238594278693\n",
      "0.00030097487615421414\n",
      "4.436564995558001e-05\n",
      "0.0011409725993871689\n",
      "0.0007892184075899422\n",
      "0.00032648263731971383\n",
      "0.00010999736696248874\n",
      "0.00035656633554026484\n",
      "0.0005139852873980999\n",
      "8.996319229481742e-05\n",
      "0.0002292779681738466\n",
      "0.0003743753186427057\n",
      "0.0004288415366318077\n",
      "0.00020733955898322165\n",
      "7.878239557612687e-05\n",
      "0.00038799349567852914\n",
      "0.0011280433973297477\n",
      "0.00045724055962637067\n",
      "8.673285628901795e-05\n",
      "0.0014853253960609436\n",
      "0.00029741122853010893\n",
      "0.00022109600831754506\n",
      "9.290855086874217e-05\n",
      "0.00020379755005706102\n",
      "0.000404429534683004\n",
      "0.00034555321326479316\n",
      "7.683435978833586e-05\n",
      "0.00047135126078501344\n",
      "0.0005688412929885089\n",
      "0.0003448162751737982\n",
      "0.00017492810729891062\n",
      "0.00038081195089034736\n",
      "9.459746070206165e-05\n",
      "0.0002521692658774555\n",
      "0.00010835466673597693\n",
      "0.0006634477758780122\n",
      "0.0004564998671412468\n",
      "0.0002673353592399508\n",
      "0.0001073295934475027\n",
      "0.0004518796631600708\n",
      "0.0004288962227292359\n",
      "5.833940304000862e-05\n",
      "0.00022598161012865603\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (value,labels) in enumerate(train_dat1):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm_ma(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op_ma.zero_grad()\n",
    "        loss.backward()\n",
    "        op_ma.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())\n",
    "    \n",
    "    for i, (value,labels) in enumerate(train_dat2):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm_ma(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op_ma.zero_grad()\n",
    "        loss.backward()\n",
    "        op_ma.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())\n",
    "    \n",
    "    for i, (value,labels) in enumerate(train_dat3):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm_ma(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op_ma.zero_grad()\n",
    "        loss.backward()\n",
    "        op_ma.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0419)\n",
      "0.1605526801002653\n",
      "0.7101374864578247\n",
      "0.15656882524490356\n",
      "0.4780701754385965\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "lstm_ma.eval()\n",
    "mse = 0\n",
    "i = 0\n",
    "re = np.array([])\n",
    "with torch.no_grad():\n",
    "    for values, labels in test_dat:\n",
    "        values = values.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = lstm_ma(values)\n",
    "        mse += torch.sum((outputs-labels)**2)\n",
    "        i += len(labels.flatten())\n",
    "        re_tmp = np.array(np.abs(labels.flatten()-outputs.flatten()))\n",
    "        re = np.concatenate((re,re_tmp),axis=0)\n",
    "print(mse/i)\n",
    "mean_rerror = np.mean(re)\n",
    "max_rerror = np.max(re)\n",
    "median_rerror = np.median(re)\n",
    "print(mean_rerror)\n",
    "print(max_rerror)\n",
    "print(median_rerror)\n",
    "idx = re <= 0.15\n",
    "idx = idx + 0\n",
    "print(sum(idx)/len(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0004)\n",
      "0.013507568038869322\n",
      "0.09291854500770569\n",
      "0.0100894495844841\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluation using ma data\n",
    "lstm_ma.eval()\n",
    "mse = 0\n",
    "i = 0\n",
    "re = np.array([])\n",
    "with torch.no_grad():\n",
    "    for values, labels in test_dat1:\n",
    "        values = values.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = lstm_ma(values)\n",
    "        mse += torch.sum((outputs-labels)**2)\n",
    "        i += len(labels.flatten())\n",
    "        re_tmp = np.array(np.abs(labels.flatten()-outputs.flatten()))\n",
    "        re = np.concatenate((re,re_tmp),axis=0)\n",
    "print(mse/i)\n",
    "mean_rerror = np.mean(re)\n",
    "max_rerror = np.max(re)\n",
    "median_rerror = np.median(re)\n",
    "print(mean_rerror)\n",
    "print(max_rerror)\n",
    "print(median_rerror)\n",
    "idx = re <= 0.15\n",
    "idx = idx + 0\n",
    "print(sum(idx)/len(idx))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If just input the original data, let's see what will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Sets\n",
    "train1 = np.array(wind1[[\"GridPower\",\"WindSpd\"]][19:])\n",
    "train_data1 = mydata(train1, train1[:,1], sequence_length)\n",
    "train2 = np.array(wind2[[\"GridPower\",\"WindSpdMA\"]][19:])\n",
    "train_data2 = mydata(train2, train2[:,1], sequence_length)\n",
    "train3 = np.array(wind3[[\"GridPower\",\"WindSpd\"]][19:])\n",
    "train_data3 = mydata(train3, train3[:,1], sequence_length)\n",
    "\n",
    "# Data Loader\n",
    "train_dat1 = data.DataLoader(dataset=train_data1, batch_size=batch_size, shuffle=True)\n",
    "train_dat2 = data.DataLoader(dataset=train_data2, batch_size=batch_size, shuffle=True)\n",
    "train_dat3 = data.DataLoader(dataset=train_data3, batch_size=batch_size, shuffle=True)\n",
    "test_dat = data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "test_dat1 = data.DataLoader(dataset=test_data1, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1531873345375061\n",
      "0.12544545531272888\n",
      "0.0009025622857734561\n",
      "0.2077295482158661\n",
      "0.07843682914972305\n",
      "0.0010841478360816836\n",
      "0.086118683218956\n",
      "0.12809598445892334\n",
      "0.0031815648544579744\n",
      "0.08365408331155777\n",
      "0.11831847578287125\n",
      "0.000769954058341682\n",
      "0.2051692008972168\n",
      "0.08468858897686005\n",
      "0.0014968625036999583\n",
      "0.1295761913061142\n",
      "0.13531726598739624\n",
      "0.0007499171188101172\n",
      "0.15072233974933624\n",
      "0.1124248281121254\n",
      "0.00595102971419692\n",
      "0.16093870997428894\n",
      "0.1186675876379013\n",
      "0.006633176002651453\n",
      "0.13628099858760834\n",
      "0.09450796991586685\n",
      "0.015297988429665565\n",
      "0.1525070071220398\n",
      "0.08831898868083954\n",
      "0.013849550858139992\n",
      "0.1587451696395874\n",
      "0.09828118234872818\n",
      "0.0028796123806387186\n",
      "0.12943123281002045\n",
      "0.0926542654633522\n",
      "0.0030538467690348625\n",
      "0.1430230438709259\n",
      "0.0819411650300026\n",
      "0.0010952467564493418\n",
      "0.24250474572181702\n",
      "0.08089534193277359\n",
      "0.0019500041380524635\n",
      "0.24121515452861786\n",
      "0.10727778822183609\n",
      "0.0010666244197636843\n",
      "0.12220825999975204\n",
      "0.08426358550786972\n",
      "0.0014305065851658583\n",
      "0.20956949889659882\n",
      "0.102442167699337\n",
      "0.0011334579903632402\n",
      "0.21401607990264893\n",
      "0.08752311766147614\n",
      "0.0018884253222495317\n",
      "0.14680470526218414\n",
      "0.09655367583036423\n",
      "0.004684622399508953\n",
      "0.17163226008415222\n",
      "0.09223904460668564\n",
      "0.006383763160556555\n",
      "0.14094670116901398\n",
      "0.1193002238869667\n",
      "0.0032496261410415173\n",
      "0.1781582087278366\n",
      "0.10271594673395157\n",
      "0.004702751059085131\n",
      "0.14076003432273865\n",
      "0.07834679633378983\n",
      "0.002870944794267416\n",
      "0.09791184961795807\n",
      "0.08957894146442413\n",
      "0.002412188798189163\n",
      "0.1700272411108017\n",
      "0.08928106725215912\n",
      "0.003302164375782013\n",
      "0.11916115134954453\n",
      "0.09666270762681961\n",
      "0.00189276784658432\n",
      "0.11594971269369125\n",
      "0.0833592340350151\n",
      "0.0022317287512123585\n",
      "0.12531794607639313\n",
      "0.11079897731542587\n",
      "0.0011657570721581578\n",
      "0.16109327971935272\n",
      "0.1252366602420807\n",
      "0.0024370583705604076\n",
      "0.18514873087406158\n",
      "0.08833211660385132\n",
      "0.000453971850220114\n",
      "0.1669015735387802\n",
      "0.10416261851787567\n",
      "0.0006922614411450922\n",
      "0.18009448051452637\n",
      "0.11447994410991669\n",
      "0.00102960504591465\n",
      "0.09660296142101288\n",
      "0.09306664764881134\n",
      "0.002239647787064314\n",
      "0.1285443753004074\n",
      "0.0873933732509613\n",
      "0.001850192085839808\n",
      "0.12542566657066345\n",
      "0.10813460499048233\n",
      "0.0027317749336361885\n",
      "0.1751789003610611\n",
      "0.09692458808422089\n",
      "0.005348438862711191\n",
      "0.17215701937675476\n",
      "0.1066652163863182\n",
      "0.002957900520414114\n",
      "0.11947337538003922\n",
      "0.07967443764209747\n",
      "0.0017116771778091788\n",
      "0.1882651299238205\n",
      "0.0784744992852211\n",
      "0.0028897218871861696\n",
      "0.11896934360265732\n",
      "0.12967434525489807\n",
      "0.002056162804365158\n",
      "0.1190938726067543\n",
      "0.11743178218603134\n",
      "0.0016921423375606537\n",
      "0.0882204994559288\n",
      "0.10421391576528549\n",
      "0.0012552604312077165\n",
      "0.12388636916875839\n",
      "0.09793852269649506\n",
      "0.0018227057298645377\n",
      "0.22079066932201385\n",
      "0.08562903106212616\n",
      "0.0016346757765859365\n",
      "0.1882796585559845\n",
      "0.09363500773906708\n",
      "0.0028056777082383633\n",
      "0.14204619824886322\n",
      "0.08925428986549377\n",
      "0.001468768692575395\n",
      "0.18664126098155975\n",
      "0.1030011922121048\n",
      "0.002617416437715292\n",
      "0.11598779261112213\n",
      "0.08585212379693985\n",
      "0.0011321225902065635\n",
      "0.15419378876686096\n",
      "0.08347578346729279\n",
      "0.0018658122280612588\n",
      "0.18332841992378235\n",
      "0.09236332774162292\n",
      "0.0027549355290830135\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (value,labels) in enumerate(train_dat1):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm_o(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op_o.zero_grad()\n",
    "        loss.backward()\n",
    "        op_o.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())\n",
    "    \n",
    "    for i, (value,labels) in enumerate(train_dat2):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm_o(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op_o.zero_grad()\n",
    "        loss.backward()\n",
    "        op_o.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())\n",
    "    \n",
    "    for i, (value,labels) in enumerate(train_dat3):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm_ma(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op_o.zero_grad()\n",
    "        loss.backward()\n",
    "        op_o.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0527)\n",
      "0.17110022030640068\n",
      "0.7066285610198975\n",
      "0.14498671144247055\n",
      "0.5184210526315789\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "lstm_ma.eval()\n",
    "mse = 0\n",
    "i = 0\n",
    "re = np.array([])\n",
    "with torch.no_grad():\n",
    "    for values, labels in test_dat:\n",
    "        values = values.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = lstm_ma(values)\n",
    "        mse += torch.sum((outputs-labels)**2)\n",
    "        i += len(labels.flatten())\n",
    "        re_tmp = np.array(np.abs(labels.flatten()-outputs.flatten()))\n",
    "        re = np.concatenate((re,re_tmp),axis=0)\n",
    "print(mse/i)\n",
    "mean_rerror = np.mean(re)\n",
    "max_rerror = np.max(re)\n",
    "median_rerror = np.median(re)\n",
    "print(mean_rerror)\n",
    "print(max_rerror)\n",
    "print(median_rerror)\n",
    "idx = re <= 0.15\n",
    "idx = idx + 0\n",
    "print(sum(idx)/len(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "sequence_length = 36\n",
    "input_size = 2\n",
    "hidden_size = 64\n",
    "num_layers = 3\n",
    "output_size = 4\n",
    "batch_size = 5\n",
    "num_epochs = 50\n",
    "learning_rate = 5e-5\n",
    "\n",
    "lstm = RNN(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "op = torch.optim.Adam(lstm_ma.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Sets\n",
    "train1 = np.array(wind1[[\"GridPowerMA\",\"WindSpdMA\"]][19:])\n",
    "train_data1 = mydata(train1, train1[:,1], sequence_length)\n",
    "train2 = np.array(wind2[[\"GridPowerMA\",\"WindSpdMA\"]][19:])\n",
    "train_data2 = mydata(train2, train2[:,1], sequence_length)\n",
    "train3 = np.array(wind3[[\"GridPowerMA\",\"WindSpdMA\"]][19:])\n",
    "train_data3 = mydata(train3, train3[:,1], sequence_length)\n",
    "\n",
    "# Test Sets\n",
    "test = np.array(wind4[[\"GridPowerMA\",\"WindSpdMA\"]][19:])\n",
    "test_target = np.array(wind4[\"GridPower\"][19:])\n",
    "test_data = mydata(test, test_target, sequence_length)\n",
    "test_data1 = mydata(test,test[:,1],sequence_length)\n",
    "\n",
    "# Data Loader\n",
    "train_dat1 = data.DataLoader(dataset=train_data1, batch_size=batch_size, shuffle=True)\n",
    "train_dat2 = data.DataLoader(dataset=train_data2, batch_size=batch_size, shuffle=True)\n",
    "train_dat3 = data.DataLoader(dataset=train_data3, batch_size=batch_size, shuffle=True)\n",
    "test_dat = data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "test_dat1 = data.DataLoader(dataset=test_data1, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09166884422302246\n",
      "0.08346317708492279\n",
      "0.08923026919364929\n",
      "0.22328858077526093\n",
      "0.18565714359283447\n",
      "0.0950552299618721\n",
      "0.20445534586906433\n",
      "0.07095950096845627\n",
      "0.08184469491243362\n",
      "0.11387506872415543\n",
      "0.14286193251609802\n",
      "0.07758806645870209\n",
      "0.1515750288963318\n",
      "0.09090658277273178\n",
      "0.07908085733652115\n",
      "0.07563485205173492\n",
      "0.15275201201438904\n",
      "0.08007799834012985\n",
      "0.14157958328723907\n",
      "0.13037031888961792\n",
      "0.0685730129480362\n",
      "0.18477240204811096\n",
      "0.14108701050281525\n",
      "0.07920848578214645\n",
      "0.11470098793506622\n",
      "0.0949801653623581\n",
      "0.07117516547441483\n",
      "0.2230275422334671\n",
      "0.14694085717201233\n",
      "0.08609166741371155\n",
      "0.13167092204093933\n",
      "0.13016214966773987\n",
      "0.0710969790816307\n",
      "0.1451786309480667\n",
      "0.11746686697006226\n",
      "0.09823349863290787\n",
      "0.2501266598701477\n",
      "0.16443754732608795\n",
      "0.07602480798959732\n",
      "0.15385909378528595\n",
      "0.10884418338537216\n",
      "0.09304346889257431\n",
      "0.3068402409553528\n",
      "0.1595802903175354\n",
      "0.08870583027601242\n",
      "0.13915421068668365\n",
      "0.16596977412700653\n",
      "0.07831870764493942\n",
      "0.16302189230918884\n",
      "0.1351991891860962\n",
      "0.08258111774921417\n",
      "0.22067317366600037\n",
      "0.09825592488050461\n",
      "0.08672767877578735\n",
      "0.1199272871017456\n",
      "0.11635024845600128\n",
      "0.07358871400356293\n",
      "0.1788535714149475\n",
      "0.12030583620071411\n",
      "0.07850901782512665\n",
      "0.13321182131767273\n",
      "0.14955677092075348\n",
      "0.0814102441072464\n",
      "0.1570371389389038\n",
      "0.13349589705467224\n",
      "0.08776421844959259\n",
      "0.1056428998708725\n",
      "0.15429484844207764\n",
      "0.08372830599546432\n",
      "0.13866639137268066\n",
      "0.10777489095926285\n",
      "0.08254964649677277\n",
      "0.24453182518482208\n",
      "0.1301785409450531\n",
      "0.07701514661312103\n",
      "0.21417264640331268\n",
      "0.13004867732524872\n",
      "0.07814536988735199\n",
      "0.24418997764587402\n",
      "0.11343935877084732\n",
      "0.08085589110851288\n",
      "0.26144760847091675\n",
      "0.08084683120250702\n",
      "0.09017878770828247\n",
      "0.11067849397659302\n",
      "0.1722479611635208\n",
      "0.07934797555208206\n",
      "0.11459394544363022\n",
      "0.11955515295267105\n",
      "0.07429730892181396\n",
      "0.2583325207233429\n",
      "0.12984511256217957\n",
      "0.07122556120157242\n",
      "0.11073155701160431\n",
      "0.15571554005146027\n",
      "0.0833650454878807\n",
      "0.20214858651161194\n",
      "0.14695724844932556\n",
      "0.0936984047293663\n",
      "0.2937154471874237\n",
      "0.13795757293701172\n",
      "0.07782679796218872\n",
      "0.2443581074476242\n",
      "0.08384181559085846\n",
      "0.08688252419233322\n",
      "0.1622665375471115\n",
      "0.12290506064891815\n",
      "0.07923775911331177\n",
      "0.26063984632492065\n",
      "0.16128934919834137\n",
      "0.08719285577535629\n",
      "0.23891031742095947\n",
      "0.15697288513183594\n",
      "0.07328297197818756\n",
      "0.13935212790966034\n",
      "0.11768170446157455\n",
      "0.07688625901937485\n",
      "0.20536835491657257\n",
      "0.17572660744190216\n",
      "0.07518135011196136\n",
      "0.20823431015014648\n",
      "0.14924249053001404\n",
      "0.07248444855213165\n",
      "0.20465297996997833\n",
      "0.19026081264019012\n",
      "0.07537084817886353\n",
      "0.25198933482170105\n",
      "0.11805503070354462\n",
      "0.09411413967609406\n",
      "0.1127157211303711\n",
      "0.19227205216884613\n",
      "0.09354645013809204\n",
      "0.16828598082065582\n",
      "0.09742207080125809\n",
      "0.07970349490642548\n",
      "0.17309270799160004\n",
      "0.09233655035495758\n",
      "0.078599713742733\n",
      "0.2105197161436081\n",
      "0.13905563950538635\n",
      "0.07430772483348846\n",
      "0.07507232576608658\n",
      "0.12420646101236343\n",
      "0.0787992775440216\n",
      "0.1448260396718979\n",
      "0.13115280866622925\n",
      "0.07111183553934097\n",
      "0.18017259240150452\n",
      "0.1943931132555008\n",
      "0.09147687256336212\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (value,labels) in enumerate(train_dat1):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op.zero_grad()\n",
    "        loss.backward()\n",
    "        op.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())\n",
    "    \n",
    "    for i, (value,labels) in enumerate(train_dat2):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op.zero_grad()\n",
    "        loss.backward()\n",
    "        op.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())\n",
    "    \n",
    "    for i, (value,labels) in enumerate(train_dat3):\n",
    "        value = value.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = lstm(value)\n",
    "        loss = criterion(outputs.squeeze(),labels.squeeze())\n",
    "\n",
    "        # optimization step\n",
    "        op.zero_grad()\n",
    "        loss.backward()\n",
    "        op.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1842)\n",
      "0.331471164982047\n",
      "1.074385404586792\n",
      "0.259888619184494\n",
      "0.3017543859649123\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "lstm.eval()\n",
    "mse = 0\n",
    "i = 0\n",
    "re = np.array([])\n",
    "with torch.no_grad():\n",
    "    for values, labels in test_dat:\n",
    "        values = values.reshape(-1,sequence_length-4,input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = lstm(values)\n",
    "        mse += torch.sum((outputs-labels)**2)\n",
    "        i += len(labels.flatten())\n",
    "        re_tmp = np.array(np.abs(labels.flatten()-outputs.flatten()))\n",
    "        re = np.concatenate((re,re_tmp),axis=0)\n",
    "print(mse/i)\n",
    "mean_rerror = np.mean(re)\n",
    "max_rerror = np.max(re)\n",
    "median_rerror = np.median(re)\n",
    "print(mean_rerror)\n",
    "print(max_rerror)\n",
    "print(median_rerror)\n",
    "idx = re <= 0.15\n",
    "idx = idx + 0\n",
    "print(sum(idx)/len(idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
